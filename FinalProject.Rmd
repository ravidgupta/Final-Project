---
title: "Predicting NBA Championship Winners Using Data from Past Championship Series"
author: "Ravi G and Rohit K"
date: "3/8/2022"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Background:
Every year, the National Basketball Association (NBA) ends the season with a series of championship games between the two best teams in the league. The series is a best-of-7, where the first team to win 4 games is the winner of the series. The championship is a very important accolade, both teams and players are compared by the number of championships they have won. In these comparisons, statistics like field goal percentage, offensive rebounds, steals, and blocks can be used to determine a team’s performance and be an indicator of how much better one team is than another. 

Statistics in the NBA are highly analyzed, very often before a championship game to try to predict the outcome of the game. Our group wants to create a model that will be able to predict whether or not a team wins the NBA championship given the team’s statistics during the NBA finals game. We want to find weights for each statistic that can show us how important each statistic is to predicting the overall winner, which helps our group conduct a greater analysis of how different focuses and strategies can affect the outcome of NBA games. Our group believes that certain statistics such as field goal percentage, turnovers, and offensive rebounds will be prevalent in winning NBA teams.

# Data:
The dataset with which we want to create our model is the NBA Finals Team Stats dataset on Kaggle uploaded by Dave Rosenman. The dataset contains final data from 1980 to 2018, and is divided into two tables. The first table contains the data of each winning team and
the second contains the losing team. Each observation includes data points like field goals made, field goals attempted, three point shots made, free throws made, total rebounds, assists, steals, turnovers, blocks, and many other statistics that will be covered in the data summary. The data takes averages from each game in the series, and its an average of the performance of the team in this category across all the games played in the series.

The NBA Finals Team Stats Dataset has been analyzed and used to create models by several Kaggle Users. One project to note is a report written by Ziyu Liu (insert citation here) called "Three pointers win championships", in which the author creates a model to see if the number of three point shots made by a team can predict whether or not the team wins the championship. In the study, the model achieves an accuracy of 59%. This tells us that, while three point shots are important, more statistics are required to be able to create a more accurate model.

In order to create the dataset we are using in this study, we started with two separate datasets, one for all of the series winners (NBA Champions) and one of the runner-ups. We created a new column ```win``` with a 1 if the team won the series and a 0 if the team lost. This will be our predictor variable for the model. Next, we combined the two datsets and randomized the order of the entries. Our goal is to first analyze each of the variables to determine which will be the most useful in creating our model, then going through several iterations of models before choosing the most accurate one. 


These are the libraries that will be used to create this model:
```{r}
library(tidyverse)
library(broom)
library(pROC)
library(plotROC)
library(rms)
library(caret)
```



## Exploratory Data Analysis

We started by creating the dataset we wish to use for this study (using the process mentioned in the Data section). 

```{r}
champs_data <- read_csv("data/champs_series_averages.csv")
runnerups_data <- read_csv("data/runner_ups_series_averages.csv")

```
```{r}
champs_data <- champs_data %>%
  mutate(win = "1")
runnerups_data <- runnerups_data %>%
  mutate(win = "0")
```
```{r}
all_data <- rbind(champs_data, runnerups_data)
```


Next, we eliminated some variables that we did not wish to explore or view the effect they would have on the model. This includes statistics like ```FTA``` (Free Throw Attempts), ```TPA``` (Three Point Attempts), ```BLK``` (Blocks). Some of these statistics describe the attempts to make a point, however the statistics describing how many points were made in that fashion would be a much more accurate tool in the model. Others simply do not happen often enough to quantifiable change the course of a championship series. 

```{r}
useful_data = subset(all_data, select = -c(...1,Year, Status, Team, FT, FTA, FTP, TRB) )
summary(useful_data$FGA)
```


Now, we can start our EDA. First, we check the plots of each variable for both the losers and the winners to make sure each distribution is roughly normal. 

Distribution of Points:
```{r, fig.dim=c(3,2)}
ggplot(data = useful_data %>% filter(win == 1), aes(x = PTS)) +   geom_histogram(binwidth = 4) + 
  labs(x = "Points", 
       y = "Count", 
       title = "Point Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = PTS)) +   geom_histogram(binwidth = 4) + 
  labs(x = "Points", 
       y = "Count", 
       title = "Point Distribution")
```

Summary of Points:
```{r}
summary(useful_data$PTS)
```
Distribution of Field Goals Made:
```{r, fig.dim=c(3,2)}
ggplot(data = useful_data %>% filter(win == 1), aes(x = FG)) +   geom_histogram(binwidth = 2) + 
  labs(x = "FG", 
       y = "Count", 
       title = "FG Made Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = FG)) +   geom_histogram(binwidth = 2) + 
  labs(x = "FG", 
       y = "Count", 
       title = "FG Made Distribution")
```
Summary of Field Goals:
```{r}
summary(useful_data$FG)
```

The distributions and summaries of the other variables can be seen in the appendix. All of the distributions appear to be somewhat normal with no outliers, We have a large sample size, so we can continue to make our model and check the residuals. 


# Creating the Model

## Model Refinement

Our model will be a binomial model (only options are 0 ad 1). The first step will be to plot the residuals of each variable in the model to check the linearity assumption. Then, we will plot the Cook's distance and remove any high-leverage points. Finally, the VIF will be checked and any variables with a high VIF will be removed from the model.

```{r}

useful_data$win <- as.factor(useful_data$win)

model <- glm(win ~ PTS + FG + FGA + FGP + TP + TPA + TPP + ORB + DRB + AST + STL + BLK + TOV + PF, useful_data, family=binomial)

summary(model)
```

```{r}

model_data <- augment(model, useful_data)
head(model_data)
```

Residual Plot of Points:
```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=.resid, y=PTS)) + geom_point() +
  labs(x="Residuals",
       y="Points",
       title="Residual Plot of Points")
```
Residual Plot of Field Goals:
```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=.resid, y=FG)) + geom_point() +
  labs(x="Residuals",
       y="Field Goals",
       title="Residual Plot of Field Goals")
```

The residual plots for each variable appear to be random and evenly dispersed (the rest can be seen in the appendix), which means that the linearity assumption is satisfied. Before we can test the accuracy of the model, we must also explore how these observations affect the model, and how the variables used in the model affect each other. First, we can plot the leverage (.cooksd) of each observation to see if there are any high-leverage data points. 

```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=seq.int(nrow(model_data)), y=.cooksd)) + geom_point() + 
  labs(x="Observation",
       y="Cook's Distance",
       title="Cook's Distance of each Observation") + 
  geom_hline(yintercept=0.125)
```

It is obvious that there are two high-leverage data points. If we use a threshold of 0.125, we can eliminate these two high-leverage points to make the model better at prediction. After this, the model must be trained on the newly-filtered data.

```{r}

filter_data <- filter(model_data, .cooksd < 0.125)

filter_data <- select(filter_data, 1:15)

filter_model <- glm(win ~ PTS + FG + FGA + FGP + TP + 
                      TPA + TPP + ORB + DRB + AST + STL + BLK + TOV + PF, filter_data, family=binomial)

summary(filter_model)

filter_data <- augment(filter_model, filter_data)

```

We must first re_train the model on the filter data The next step is to check how the variables interact with each other. To measure this, we want to calculate the Variable Inflation Factor, or $VIF$. 

```{r}

vif(filter_model)

```

Some of the variables have a very high $VIF$, so we can only include certain variables to keep the score lower. To see which variables are better included and not included, we must create multiple models and assess which one has the best accuracy. We can leave all of the variables who's $VIF$ is lower than 10, but the others must be removed for higher accuracy. We will create three models, one that will include only the Field Goals made and the Three Point shots made; one that will only include the Field Goal percentage and Three Point percentage; and finally an attempts model that will include the number of Field Goal attempts and Three Point attempts. 

```{r}

points_model <- glm(win ~ PTS + FG + TP + ORB + DRB + AST + STL + BLK + TOV + PF, 
                    data = filter_data, na.action = na.omit, family = binomial)

percentage_model <- glm(win ~ PTS + FGP + TPP + ORB + DRB + AST + STL + BLK + TOV + PF, 
                    data = filter_data, na.action = na.omit, family = binomial)

attempts_model <- glm(win ~ PTS + FGA + TPA + ORB + DRB + AST + STL + BLK + TOV + PF, 
                    data = filter_data, na.action = na.omit, family = binomial)
```

Since we created three new models, we need to repeat the same steps we did before with our previous model. We need to check for high leverage points and the Variable Inflation Factor.

Points Model:
```{r}
points_data <- filter_data
ggplot(data = points_model, aes(x=seq.int(nrow(points_data)), y=.cooksd)) + geom_point() + 
  labs(x="Observation",
       y="Cook's Distance",
       title="Cook's Distance of each Observation") + 
  geom_hline(yintercept=0.125)
```
```{r}
points_data <- filter(points_data, .cooksd < 0.125)

points_data <- select(points_data, 1:15)

points_model <- glm(win ~ PTS + FG + TP + ORB + DRB + AST + STL + BLK + TOV + PF, points_data, family=binomial)

summary(points_model)

points_data <- augment(points_model, points_data)
```
```{r}
vif(points_model)
```

Percentage Model:
```{r}
percentage_data <- filter_data
ggplot(data = percentage_model, aes(x=seq.int(nrow(percentage_data)), y=.cooksd)) + geom_point() + 
  labs(x="Observation",
       y="Cook's Distance",
       title="Cook's Distance of each Observation") + 
  geom_hline(yintercept=0.125)
```

```{r}
percentage_data <- filter(percentage_data, .cooksd < 0.125)

percentage_data <- select(percentage_data, 1:15)

percentage_model <- glm(win ~ PTS + FGP + TPP + ORB + DRB + AST + STL + BLK + TOV + PF, percentage_data, family=binomial)

summary(percentage_model)

percentage_data <- augment(percentage_model, percentage_data)
```
```{r}
vif(percentage_model)
```

Attempts Model:
```{r}
attempts_data <- filter_data
ggplot(data = attempts_model, aes(x=seq.int(nrow(attempts_data)), y=.cooksd)) + geom_point() + 
  labs(x="Observation",
       y="Cook's Distance",
       title="Cook's Distance of each Observation") + 
  geom_hline(yintercept=0.125)
```
```{r}
attempts_data <- filter(attempts_data, .cooksd < 0.125)

attempts_data <- select(attempts_data, 1:15)

attempts_model <- glm(win ~ PTS + FGA + TPA + ORB + DRB + AST + STL + BLK + TOV + PF, attempts_data, family=binomial)

summary(attempts_model)

attempts_data <- augment(attempts_model, attempts_data)
```
```{r}
vif(attempts_model)
```

To evaluate the accuracy of each model, we will train each model to our dataset to make predictions and measure the accuracy of these predictions. To do this, for each model we will plot the ROC curve, find the ideal threshold of each model, create predictions, then measure the accuracy. 

Points Model:
```{r, fig.dim=c(3,2)}

points_roc <- roc(points_data, win, .fitted, plot=TRUE)

```
```{r}

threshold <- coords(points_roc, "best", ret = "threshold")
print(threshold)
```

The ideal threshold for the points model has been shown as ```0.2346563```. Using this threshold, we can create a confusion matrix and draw conclusions about the accuracy of the model. 

```{r}
points_data <- mutate(points_data, pred = ifelse(.fitted > 0.2346563, 1, 0))

points_data$pred <- as.factor(points_data$pred)

confusionMatrix(points_data$pred, points_data$win)

```

Percentage Model:
```{r, fig.dim=c(3,2)}

percentage_roc <- roc(percentage_data, win, .fitted, plot=TRUE)

```
```{r}

threshold <- coords(percentage_roc, "best", ret = "threshold")
print(threshold)
```

The ideal threshold for the percentage model has been shown as ```0.4107141```. Using this threshold, we can create a confusion matrix and draw conclusions about the accuracy of the model. 

```{r}
percentage_data <- mutate(percentage_data, pred = ifelse(.fitted > 0.4107141, 1, 0))

percentage_data$pred <- as.factor(percentage_data$pred)

confusionMatrix(percentage_data$pred, percentage_data$win)

```

Attempts Model:
```{r, fig.dim=c(3,2)}

attempts_roc <- roc(attempts_data, win, .fitted, plot=TRUE)

```
```{r}

threshold <- coords(attempts_roc, "best", ret = "threshold")
print(threshold)
```

The ideal threshold for the percentage model has been shown as ```-1.059852```. Using this threshold, we can create a confusion matrix and draw conclusions about the accuracy of the model. 

```{r}
attempts_data <- mutate(attempts_data, pred = ifelse(.fitted > -1.059852, 1, 0))

attempts_data$pred <- as.factor(attempts_data$pred)

confusionMatrix(attempts_data$pred, attempts_data$win)

```
The accuracy of the points model was found to be 72.46%. Using this same method for the other models, we found that the accuracy of the percentage model was 84.06% and the accuracy of the attempts model was 85.51%. In this case, solely using the attempts of field goals and three pointers made the model more accurate. We will use the attempts model as our current model for the next step of tests.
Since the VIF was close to 10 for all the models with points, we are going to try a model without PTS. We will use the attempts model since it produced the highest accuracy from our previous models

No points Model:
```{r}
nopoints_model <- glm(win ~ FGA + TPA + ORB + DRB + AST + STL + BLK + TOV + PF, 
                    data = filter_data, na.action = na.omit, family = binomial)
```
```{r}
nopoints_data <- filter_data
ggplot(data = nopoints_model, aes(x=seq.int(nrow(nopoints_data)), y=.cooksd)) + geom_point() + 
  labs(x="Observation",
       y="Cook's Distance",
       title="Cook's Distance of each Observation") + 
  geom_hline(yintercept=0.125)
```
```{r}
nopoints_data <- filter(nopoints_data, .cooksd < 0.125)

nopoints_data <- select(nopoints_data, 1:15)

nopoints_model <- glm(win ~ FGA + TPA + ORB + DRB + AST + STL + BLK + TOV + PF, nopoints_data, family=binomial)

summary(nopoints_model)

nopoints_data <- augment(nopoints_model, nopoints_data)
```
```{r}
vif(nopoints_model)
```
```{r, fig.dim=c(3,2)}

nopoints_roc <- roc(nopoints_data, win, .fitted, plot=TRUE)

```
```{r}

threshold <- coords(nopoints_roc, "best", ret = "threshold")
print(threshold)
```
```{r}
nopoints_data <- mutate(nopoints_data, pred = ifelse(.fitted > -0.3855574, 1, 0))

nopoints_data$pred <- as.factor(nopoints_data$pred)

confusionMatrix(nopoints_data$pred, nopoints_data$win)

```

The accuracy of this model is 87.84%. This model provided the best accuracy and passed all the assumptions. We decided that this model will be our final model and we can make conclusions based on this model.

```{r}
final_model <- attempts_model
```

# Conclusion

```{r}
summary(final_model)
```

By using this model, we can predict whether or not an NBA team won a finals series given their statistics from the series with an accuracy of 87.84%, making the model useful (better than guessing win or lose for each year). 

## Observations

We found it interesting that the attempts of field goals and three pointers provided the most accurate model for predicting who won the series. It intuitively makes sense that the field goals scored matter more than attempts because basketball games are decided by the score, not the attempts; however, attempts might signal how ineffective a team's offense really can be.

It is worth noting that, while the coefficients for most variables make sense (ie. turnovers having a negative coefficient and points having a positive coefficient), others have a surprising effect on the model. 

For example, the coefficients for field goals attempted (```FGA```) is negative, implying that more field goal attempts indicate a lesser likelihood that a team won the game. This can be attributed to the fact that basketball is about making baskets rather than taking shots. If you aren't making these shots you are taking, you most likely won't win the game. 

Some variables with high positive coefficients include: Points (```ORB```), Offensive Rebounds (```ORB```), Defensive Rebounds (```DRB```), and Steals (```STL```). These all seem to be good at indicating whether or not teams won the series, which is fascinating because they cover different aspects of the game. A team cannot solely rely on shooting, defense, or size (in the case of offensive and defensive rebounds) to win a championship; they must have all aspects of the game.

The two variables with low positive coefficients include: Field Goals Attempted (```FGA```) and Turnovers (```TOV```). These also seem good in indicating whether or not teams won the series. Having low turnovers is good as it will give you more opportunities to shoot the ball and prevent free baskets. The other variables don't have that big of an impact on the final result, but that doesn't mean they don't matter at all.


# Appendix

## Distributions of variables for Winners and Losers

Distribution of Field Goals Attemped:
```{r, fig.dim=c(3,2)}
ggplot(data = useful_data %>% filter(win == 1), aes(x = FGA)) +   geom_histogram(binwidth = 3) + 
  labs(x = "FGA", 
       y = "Count", 
       title = "FG Attempted Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = FGA)) +   geom_histogram(binwidth = 3) + 
  labs(x = "FGA", 
       y = "Count", 
       title = "FG Attempted Distribution")
```
Summary of Field Goal Attempts:
```{r}
summary(useful_data$FGA)
```
Distribution of Field Goal Percentage:
```{r, fig.dim=c(3,2)}
ggplot(data = useful_data %>% filter(win == 1), aes(x = FGP)) +   geom_histogram(binwidth = 2) + 
  labs(x = "FGP", 
       y = "Count", 
       title = "FG Percentage Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = FGP)) +   geom_histogram(binwidth = 2) + 
  labs(x = "FGP", 
       y = "Count", 
       title = "FG Percentage Distribution")
```
Summary of Field Goal Percentage:
```{r}
summary(useful_data$FGP)
```
Distribution of Three Point Distribution:
```{r, fig.dim=c(3,2)}
ggplot(data = useful_data %>% filter(win == 1), aes(x = TP)) +   geom_histogram(binwidth = 2) + 
  labs(x = "TP", 
       y = "Count", 
       title = "Three Point Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = TP)) +   geom_histogram(binwidth = 2) + 
  labs(x = "TP", 
       y = "Count", 
       title = "Three Point Distribution")
```
Summary of Three Point:
```{r}
summary(useful_data$TP)
```
Distribution of Three Point Attempted:
```{r, fig.dim=c(3,2)}
ggplot(data = useful_data %>% filter(win == 1), aes(x = TPA)) +   geom_histogram(binwidth = 3) + 
  labs(x = "TPA", 
       y = "Count", 
       title = "TP Attempted Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = TPA)) +   geom_histogram(binwidth = 3) + 
  labs(x = "TPA", 
       y = "Count", 
       title = "TP Attempted Distribution")
```
Summary of Three Point Attempts:
```{r}
summary(useful_data$TPA)
```
Distribution of Three Point Percentage:
```{r, fig.dim=c(3,2)}
ggplot(data = useful_data %>% filter(win == 1), aes(x = TPP)) +   geom_histogram(binwidth = 4) + 
  labs(x = "TP Percentage", 
       y = "Count", 
       title = "TP Percentage Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = TPP)) +   geom_histogram(binwidth = 4) + 
  labs(x = "TP Percentage", 
       y = "Count", 
       title = "TP Percentage Distribution")
```
Summary of Three Point Percentage:
```{r}
summary(useful_data$TPP)
```
Distribution of Offensive Rebounds:
```{r, fig.dim=c(3,2)}
ggplot(data = useful_data %>% filter(win == 1), aes(x = ORB)) +   geom_histogram(binwidth = 2) + 
  labs(x = "ORB", 
       y = "Count", 
       title = "Offensive Rebounds Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = ORB)) +   geom_histogram(binwidth = 2) + 
  labs(x = "ORB", 
       y = "Count", 
       title = "Offensive Rebounds Distribution")
```
Summary of Offensive Rebounds:
```{r}
summary(useful_data$ORB)
```
Distribution of Defensive Rebounds:
```{r, fig.dim=c(3,2)}
ggplot(data = useful_data %>% filter(win == 1), aes(x = DRB)) +   geom_histogram(binwidth = 2) + 
  labs(x = "DRB", 
       y = "Count", 
       title = "Defensive Rebound Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = DRB)) +   geom_histogram(binwidth = 2) + 
  labs(x = "DRB", 
       y = "Count", 
       title = "Defensive Rebound Distribution")
```
Summary of Defensive Rebounds:
```{r}
summary(useful_data$DRB)
```
Distribution of Assists:
```{r, fig.dim=c(3,2)}
ggplot(data = useful_data %>% filter(win == 1), aes(x = AST)) +   geom_histogram(binwidth = 2) + 
  labs(x = "AST", 
       y = "Count", 
       title = "Assists Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = AST)) +   geom_histogram(binwidth = 2) + 
  labs(x = "AST", 
       y = "Count", 
       title = "Assists Distribution")
```
Summary of Assists:
```{r}
summary(useful_data$AST)
```
Distribution of Steals:
```{r, fig.dim=c(3,2)}
ggplot(data = useful_data %>% filter(win == 1), aes(x = STL)) +   geom_histogram(binwidth = 1) + 
  labs(x = "STL", 
       y = "Count", 
       title = "Steals Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = STL)) +   geom_histogram(binwidth = 1) + 
  labs(x = "STL", 
       y = "Count", 
       title = "Steals Distribution")
```
Summary of Steals:
```{r}
summary(useful_data$STL)
```
Distribution of Blocks:
```{r, fig.dim=c(3,2)}
ggplot(data = useful_data %>% filter(win == 1), aes(x = BLK)) +   geom_histogram(binwidth = 1) + 
  labs(x = "BLK", 
       y = "Count", 
       title = "Block Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = BLK)) +   geom_histogram(binwidth = 1) + 
  labs(x = "BLK", 
       y = "Count", 
       title = "Block Distribution")
```
Summary of Blocks:
```{r}
summary(useful_data$BLK)
```
Distribution of Turnovers:
```{r, fig.dim=c(3,2)}
ggplot(data = useful_data %>% filter(win == 1), aes(x = TOV)) +   geom_histogram(binwidth = 2) + 
  labs(x = "TOV", 
       y = "Count", 
       title = "Turnovers Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = TOV)) +   geom_histogram(binwidth = 2) + 
  labs(x = "TOV", 
       y = "Count", 
       title = "Turnovers Distribution")
```
Summary of Turnovers:
```{r}
summary(useful_data$TOV)
```
Distribution of Personal Fouls:
```{r, fig.dim=c(3,2)}
ggplot(data = useful_data %>% filter(win == 1), aes(x = PF)) +   geom_histogram(binwidth = 2) + 
  labs(x = "PF", 
       y = "Count", 
       title = "Personal Foul Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = PF)) +   geom_histogram(binwidth = 2) + 
  labs(x = "PF", 
       y = "Count", 
       title = "Personal Foul Distribution")
```
Summary of Personal Fouls:
```{r}
summary(useful_data$PF)
```

## Residuals of Each Variable
Residual Plot of Field Goal Attempts:
```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=.resid, y=FGA)) + geom_point() +
  labs(x="Residuals",
       y="Field Goal Attemps",
       title="Residual Plot of Field Goal Attempts")
```
Residual Plot of Points:
```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=.resid, y=FGP)) + geom_point() +
  labs(x="Residuals",
       y="Field Goal Percentage",
       title="Residual Plot of Points")
```
Residual Plot of Three Pointers:
```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=.resid, y=TP)) + geom_point() +
  labs(x="Residuals",
       y="Three Pointers",
       title="Residual Plot of Three Pointers")
```
Residual Plot of Three Point Attempts:
```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=.resid, y=TPA)) + geom_point() +
  labs(x="Residuals",
       y="Three Point Attempts",
       title="Residual Plot of Three Point Attemps")
```
Residual Plot of Three Point Percentage:
```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=.resid, y=TPP)) + geom_point() +
  labs(x="Residuals",
       y="Three Point Percentage",
       title="Residual Plot of Three Point Percentage")
```
Residual Plot of Offensive Rebounds:
```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=.resid, y=ORB)) + geom_point() +
  labs(x="Residuals",
       y="Offensive Rebounds",
       title="Residual Plot of Offensive Rebounds")
```
Residual Plot of Defensive Rebounds:
```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=.resid, y=DRB)) + geom_point() +
  labs(x="Residuals",
       y="Defensive Rebounds",
       title="Residual Plot of Defensive Rebounds")
```
Residual Plot of Assists:
```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=.resid, y=AST)) + geom_point() +
  labs(x="Residuals",
       y="Assists",
       title="Residual Plot of Assists")
```
Residual Plot of Steals:
```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=.resid, y=STL)) + geom_point() +
  labs(x="Residuals",
       y="Steals",
       title="Residual Plot of Steals")
```
Residual Plot of Blocks:
```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=.resid, y=BLK)) + geom_point() +
  labs(x="Residuals",
       y="Blocks",
       title="Residual Plot of Blocks")
```
Residual Plot of Turnovers:
```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=.resid, y=TOV)) + geom_point() +
  labs(x="Residuals",
       y="Turnovers",
       title="Residual Plot of Turnovers")
```
Residual Plot of PF:
```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=.resid, y=PF)) + geom_point() +
  labs(x="Residuals",
       y="PF",
       title="Residual Plot of PF")
```
