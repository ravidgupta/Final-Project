---
title: "What Statistics Make a Team NBA Champions"
author: "Ravi G and Rohit K"
date: "3/8/2022"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Background:
Every year, the National Basketball Association (NBA) ends the season with a series of championship games between the two best teams in the league. The series is a best-of-7, where the first team to win 4 games is the winner of the series. The championship is a very important accolade, both teams and players are compared by the number of championships they have won. In these comparisons, statistics like field goal percentage, offensive rebounds, steals, and blocks can be used to determine a teamâ€™s performance and be an indicator of how much better one team is than another. 

Statistics in the NBA are highly analyzed, very often with the goal of assessing which areas a team can improve upon. Our group wants to create a model that will be able to tell us which key statistics from the NBA Championship series from 1980-2018 were important in deciding the championship. More specifically, we want to find weights for each statistic that can show us how important each statistic is to predicting the overall winner, which helps our group conduct a greater analysis of how different focuses and strategies can affect the outcome of NBA games. 

We will be using the NBA Finals Team Stats Dataset, which has been analyzed and used to create models by several Kaggle Users. One project to note is a report written by Ziyu Liu called "Three pointers win championships", in which the author creates a model to see if the number of three point shots made by a team can predict whether or not the team wins the championship. In the study, the model achieves an accuracy of 59%. This tells us that, while three point shots are important, more statistics are required to be able to create a more accurate model.

Another example of analyzing NBA championship data comes from the article "Stat of the Week: How champions are built in the NBA" by Ryan Blackburn. In this article, Blackburn specifically analyzes the Denver Nuggets, and why the team has never won an NBA championship. In the paper, Blackburn ranks teams by offense, defense, and net-rating, and points out that only five out of past 20 championship winners have ranked outside of the top 3 teams in net-rating. This leads us to believe that a team needs both impressive defensive statistics and offensive statistics in order to win an NBA championship.

Given these studies and our group's own knowledge of the NBA, we believe that both offensive statistics like field goals, three pointers, offensive rebounds; and defensive statistics like steals and defensive rebounds will have a high impact on whether or not a team wins the championship.

# Data:
The dataset with which we want to create our model is the NBA Finals Team Stats dataset on Kaggle uploaded by Dave Rosenman. The dataset contains final data from 1980 to 2018, and is divided into two tables. The first table contains the data of each winning team and
the second contains the losing team. Each observation includes data points like field goals made, field goals attempted, three point shots made, free throws made, total rebounds, assists, steals, turnovers, blocks, and many other statistics that will be covered in the data summary. The data takes averages from each game in the series, and its an average of the performance of the team in this category across all the games played in the series.


In order to create the dataset we are using in this study, we started with two separate datasets, one for all of the series winners (NBA Champions) and one of the runner-ups. We created a new column ```win``` with a 1 if the team won the series and a 0 if the team lost. This will be our predictor variable for the model. Next, we combined the two datsets and randomized the order of the entries. Our goal is to first analyze each of the variables to determine which will be the most useful in creating our model, then going through several iterations of models before choosing the most accurate one. 


```{r, include=FALSE}
library(tidyverse)
library(broom)
library(pROC)
library(plotROC)
library(rms)
library(caret)
library(skimr)
library(gridExtra)
library(knitr)
```



## Exploratory Data Analysis

```{r, include=FALSE}
champs_data <- read_csv("data/champs_series_averages.csv")
runnerups_data <- read_csv("data/runner_ups_series_averages.csv")

```
```{r, include=FALSE}
champs_data <- champs_data %>%
  mutate(win = "1")
runnerups_data <- runnerups_data %>%
  mutate(win = "0")
```
```{r, include=FALSE}
all_data <- rbind(champs_data, runnerups_data)
```


As mentioned before, we combined the datasets and added a ```win``` variable to tell us whether the team won a championship or not. Since there were over 20 variables, our goal was to choose key variables to analyze that we knew would have an impact on the game. To do this, we eliminated some variables that we did not wish to explore or view the effect they would have on the model. This includes statistics like ```FTA``` (Free Throw Attempts), ```BLK``` (Blocks). While in the game this statistics might be important, having a statistic like Free Throw Attemtps or Blocks without any context about the other team's performance in the same category would most likely not be useful. 

```{r, include=FALSE}
useful_data = subset(all_data, select = -c(...1,Year, Status, Team, FT, FTA, FTP, TRB) )

```


Now, we can start our EDA. First, we check the plots of each variable for both the losers and the winners to make sure each distribution is roughly normal. 

Distribution of Points:

```{r, fig.dim=c(3,2), echo=FALSE}
ggplot(data = useful_data %>% filter(win == 1), aes(x = PTS)) +   geom_histogram(binwidth = 4) + 
  labs(x = "Points", 
       y = "Count", 
       title = "Point Distribution for Champs")
ggplot(data = useful_data %>% filter(win == 0), aes(x = PTS)) +   geom_histogram(binwidth = 4) + 
  labs(x = "Points", 
       y = "Count", 
       title = "Point Distribution for RunnerUps")
```

Summary of Points:

```{r, echo=FALSE}
summary(useful_data$PTS)
```

Distribution of Field Goals Made:

```{r, fig.dim=c(3,2), echo=FALSE}
ggplot(data = useful_data %>% filter(win == 1), aes(x = FG)) +   geom_histogram(binwidth = 2) + 
  labs(x = "FG", 
       y = "Count", 
       title = "FG Made Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = FG)) +   geom_histogram(binwidth = 2) + 
  labs(x = "FG", 
       y = "Count", 
       title = "FG Made Distribution")
```

Summary of Field Goals:

```{r, echo=FALSE}
summary(useful_data$FG)
```

The distributions and summaries of the other variables can be seen in the appendix. All of the distributions appear to be somewhat normal with no outliers, We have a large sample size, so we can continue to make our model and check the residuals. 


# Creating the Model

## Model Refinement

Our model will be a binomial model (only options are 0 ad 1). The first step will be to plot the residuals of each variable in the model to check the linearity assumption. Then, we will plot the Cook's distance and remove any high-leverage points. Finally, the VIF will be checked and any variables with a high VIF will be removed from the model.

Summary of Model:

```{r, echo=FALSE}

useful_data$win <- as.factor(useful_data$win)

model <- glm(win ~ PTS + FG + FGA + FGP + TP + TPA + TPP + ORB + DRB + AST + STL + BLK + TOV + PF, useful_data, family=binomial)

tidy(model, conf.int = TRUE, exponentiate = FALSE) %>%
  kable(digits = 3, format = "markdown")
```

```{r, include=FALSE}

model_data <- augment(model, useful_data)

```

Residual Plot of Points:

```{r, fig.dim=c(3,2), echo=FALSE}
ggplot(data = model_data, aes(x=.resid, y=PTS)) + geom_point() +
  labs(x="Residuals",
       y="Points",
       title="Residual Plot of Points")
```

Residual Plot of Field Goals:

```{r, fig.dim=c(3,2), echo=FALSE}
ggplot(data = model_data, aes(x=.resid, y=FG)) + geom_point() +
  labs(x="Residuals",
       y="Field Goals",
       title="Residual Plot of Field Goals")
```

The residual plots for each variable appear to be random and evenly dispersed (the rest can be seen in the appendix), which means that the linearity assumption is satisfied. Before we can test the accuracy of the model, we must also explore how these observations affect the model, and how the variables used in the model affect each other. First, we can plot the leverage (.cooksd) of each observation to see if there are any high-leverage data points. 

```{r, fig.dim=c(3,2), echo=FALSE}
ggplot(data = model_data, aes(x=seq.int(nrow(model_data)), y=.cooksd)) + geom_point() + 
  labs(x="Observation",
       y="Cook's Distance",
       title="Cook's Distance of each Observation") + 
  geom_hline(yintercept=0.125)
```

It is obvious that there are two high-leverage data points. If we use a threshold of 0.125, we can eliminate these two high-leverage points to make the model better at prediction. After this, the model must be trained on the newly-filtered data. Then, we re-train our model on the now filtered dataset. 

Summary of Model on Filtered Dataset:

```{r, echo=FALSE}

filter_data <- filter(model_data, .cooksd < 0.125)

filter_data <- select(filter_data, 1:15)

filter_model <- glm(win ~ PTS + FG + FGA + FGP + TP + 
                      TPA + TPP + ORB + DRB + AST + STL + BLK + TOV + PF, filter_data, family=binomial)

tidy(filter_model, conf.int = TRUE, exponentiate = FALSE) %>%
  kable(digits = 3, format = "markdown")

filter_data <- augment(filter_model, filter_data)

```

The next step is to check how the variables interact with each other. To measure this, we want to calculate the Variable Inflation Factor, or $VIF$. 

```{r, echo=FALSE}

vif(filter_model)

```

Some of the variables have a very high $VIF$, so we can only include certain variables to keep the score lower. To see which variables are better included and not included, we must create multiple models and assess which one has the best accuracy. We can leave all of the variables who's $VIF$ is lower than 10, but the others must be removed for higher accuracy. We will create three models, one that will include only the Field Goals made and the Three Point shots made; one that will only include the Field Goal percentage and Three Point percentage; and finally an attempts model that will include the number of Field Goal attempts and Three Point attempts. The, we check the VIF of each model to make sure that it has been reduced (that each variable has a $VIF$ lower than 10)

```{r, include=FALSE}

points_model <- glm(win ~ PTS + FG + TP + ORB + DRB + AST + STL + BLK + TOV + PF, 
                    data = filter_data, na.action = na.omit, family = binomial)

percentage_model <- glm(win ~ PTS + FGP + TPP + ORB + DRB + AST + STL + BLK + TOV + PF, 
                    data = filter_data, na.action = na.omit, family = binomial)

attempts_model <- glm(win ~ PTS + FGA + TPA + ORB + DRB + AST + STL + BLK + TOV + PF, 
                    data = filter_data, na.action = na.omit, family = binomial)
```


Points Model:

```{r, echo=FALSE}
vif(points_model)
```

Percentage Model:

```{r, echo=FALSE}
vif(percentage_model)
```

Attempts Model:

```{r, echo=FALSE}
vif(attempts_model)
```

To evaluate the accuracy of each model, we will train each model to our dataset to make predictions and measure the accuracy of these predictions. We believe that the best way to evaluate the models is to calculate the $AIC$, or the Akaike information criterion.  

Points Model:

```{r, echo=FALSE}

AIC(points_model)

```

Percentage Model:

```{r, echo=FALSE}

AIC(percentage_model)

```


Attempts Model:

```{r, echo=FALSE}

AIC(attempts_model)
```


We can see that the AIC of the attempts model is significantly lower than the AIC of the other two models. Since a lower AIC is the result of a better fitting model for the data, we will use only the attempts model in the process of refining our model. 

Since the VIF of the model was close to 10, we want to check whether or not including the ```PTS``` variable in the model makes the model better or worse. To do this, we will create two models from our attempts model, the first including points and the second not including points. We will chose whichever model has the lower AIC to be our final model.

AIC of No Points Model:

```{r, include=FALSE}
nopoints_model <- glm(win ~ FGA + TPA + ORB + DRB + AST + STL + BLK + TOV + PF, 
                    data = filter_data, na.action = na.omit, family = binomial)
```

```{r, echo=FALSE}
AIC(nopoints_model)
```

The AIC of the attempts model has already been calculated (```79.57669```), so we know that the attempts model is a better fit so we will be keeping PTS in our model. After conducting backwards selection, we removed all of the insignificant variables in our model. We will create our final model with significant variables.

```{r, include=FALSE}
backwards_model <- glm(win ~ PTS + FGA + ORB + DRB + STL, 
                    data = filter_data, na.action = na.omit, family = binomial)
summary(backwards_model)
```

```{r, echo=FALSE}
vif(backwards_model)
```

```{r, echo=FALSE}
AIC(backwards_model)
```

The VIF of our model has all the variables under 10, so our model does not have multicollinearity. The AIC of the attempts model has been calculated at (```77.37076```), so we know this model is the best fit we have so far. This will be our final model.

# Limitations and Conclusion

```{r, echo=FALSE}
final_model <- backwards_model
tidy(final_model, conf.int = TRUE, exponentiate = FALSE) %>%
  kable(digits = 3, format = "markdown")
```

Our final model had an AIC of 77.37076. Using the coefficients of the final model, we can see that all five of these variables are the most important factors in whether or not a team from 1980-2018 won the NBA championship. It is important to note that, because this data does not cover years in the NBA before 1980 and after 2018, that using the model to draw conclusions about NBA championships in those years might not have accurate results due to changes in how the NBA plays basketball over time. Also, this model is not meant to serve as a predictor for who will win future NBA championships, only to analyze statistics from championship series' covered by the data.

# Observations

We found it interesting that the attempts  provided the most accurate model for predicting who won the series. It intuitively makes sense that the field goals or three pointers scored matter more than attempts because basketball games are decided by the score, not the attempts; however, attempts might signal how ineffective a team's offense really can be.

It is worth noting that, while the coefficients for most variables make sense (ie. turnovers having a negative coefficient and points having a positive coefficient), others have a surprising effect on the model. 

For example, the coefficients for field goals attempted (```FGA```) is negative, implying that more field goal attempts indicate a lesser likelihood that a team won the game. This can be attributed to the fact that basketball is about making baskets rather than taking shots. If you aren't making these shots you are taking, you most likely won't win the game. 

Some variables with high positive coefficients include: Offensive Rebounds (```ORB```), Defensive Rebounds (```DRB```), and Steals (```STL```). These all seem to be good at indicating whether or not teams won the series, which is fascinating because they cover different aspects of the game. A team cannot solely rely on shooting, defense, or size (in the case of offensive and defensive rebounds) to win a championship; they must have all aspects of the game.

The only variable with a low negative coefficient that we kept in our model was Field Goals Attempted (```FGA```). This also seems good in indicating whether or not teams won the series. Having low field goals attempted most likely means you made more of your baskets. The other variables don't have that big of an impact on the final result, but that does not mean those variables do not impact the final result of a championship.


# Appendix

## Distributions of variables for Winners and Losers

Distribution of Field Goals Attempted:

```{r, fig.dim=c(3,2), echo=FALSE}
ggplot(data = useful_data %>% filter(win == 1), aes(x = FGA)) +   geom_histogram(binwidth = 3) + 
  labs(x = "FGA", 
       y = "Count", 
       title = "FG Attempted Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = FGA)) +   geom_histogram(binwidth = 3) + 
  labs(x = "FGA", 
       y = "Count", 
       title = "FG Attempted Distribution")
```

Summary of Field Goal Attempts:

```{r, echo=FALSE}
summary(useful_data$FGA)
```

Distribution of Field Goal Percentage:

```{r, fig.dim=c(3,2), echo=FALSE}
ggplot(data = useful_data %>% filter(win == 1), aes(x = FGP)) +   geom_histogram(binwidth = 2) + 
  labs(x = "FGP", 
       y = "Count", 
       title = "FG Percentage Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = FGP)) +   geom_histogram(binwidth = 2) + 
  labs(x = "FGP", 
       y = "Count", 
       title = "FG Percentage Distribution")
```

Summary of Field Goal Percentage:

```{r, echo=FALSE}
summary(useful_data$FGP)
```

Distribution of Three Point Distribution:

```{r, fig.dim=c(3,2), echo=FALSE}
ggplot(data = useful_data %>% filter(win == 1), aes(x = TP)) +   geom_histogram(binwidth = 2) + 
  labs(x = "TP", 
       y = "Count", 
       title = "Three Point Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = TP)) +   geom_histogram(binwidth = 2) + 
  labs(x = "TP", 
       y = "Count", 
       title = "Three Point Distribution")
```

Summary of Three Point:

```{r, echo=FALSE}
summary(useful_data$TP)
```

Distribution of Three Point Attempted:

```{r, fig.dim=c(3,2), echo=FALSE}
ggplot(data = useful_data %>% filter(win == 1), aes(x = TPA)) +   geom_histogram(binwidth = 3) + 
  labs(x = "TPA", 
       y = "Count", 
       title = "TP Attempted Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = TPA)) +   geom_histogram(binwidth = 3) + 
  labs(x = "TPA", 
       y = "Count", 
       title = "TP Attempted Distribution")
```

Summary of Three Point Attempts:

```{r, echo=FALSE}
summary(useful_data$TPA)
```

Distribution of Three Point Percentage:

```{r, fig.dim=c(3,2), echo=FALSE}
ggplot(data = useful_data %>% filter(win == 1), aes(x = TPP)) +   geom_histogram(binwidth = 4) + 
  labs(x = "TP Percentage", 
       y = "Count", 
       title = "TP Percentage Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = TPP)) +   geom_histogram(binwidth = 4) + 
  labs(x = "TP Percentage", 
       y = "Count", 
       title = "TP Percentage Distribution")
```

Summary of Three Point Percentage:

```{r, echo=FALSE}
summary(useful_data$TPP)
```

Distribution of Offensive Rebounds:

```{r, fig.dim=c(3,2), echo=FALSE}
ggplot(data = useful_data %>% filter(win == 1), aes(x = ORB)) +   geom_histogram(binwidth = 2) + 
  labs(x = "ORB", 
       y = "Count", 
       title = "Offensive Rebounds Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = ORB)) +   geom_histogram(binwidth = 2) + 
  labs(x = "ORB", 
       y = "Count", 
       title = "Offensive Rebounds Distribution")
```

Summary of Offensive Rebounds:

```{r, echo=FALSE}
summary(useful_data$ORB)
```

Distribution of Defensive Rebounds:

```{r, fig.dim=c(3,2), echo=FALSE}
ggplot(data = useful_data %>% filter(win == 1), aes(x = DRB)) +   geom_histogram(binwidth = 2) + 
  labs(x = "DRB", 
       y = "Count", 
       title = "Defensive Rebound Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = DRB)) +   geom_histogram(binwidth = 2) + 
  labs(x = "DRB", 
       y = "Count", 
       title = "Defensive Rebound Distribution")
```

Summary of Defensive Rebounds:

```{r, echo=FALSE}
summary(useful_data$DRB)
```

Distribution of Assists:

```{r, fig.dim=c(3,2), echo=FALSE}
ggplot(data = useful_data %>% filter(win == 1), aes(x = AST)) +   geom_histogram(binwidth = 2) + 
  labs(x = "AST", 
       y = "Count", 
       title = "Assists Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = AST)) +   geom_histogram(binwidth = 2) + 
  labs(x = "AST", 
       y = "Count", 
       title = "Assists Distribution")
```

Summary of Assists:

```{r, echo=FALSE}
summary(useful_data$AST)
```

Distribution of Steals:

```{r, fig.dim=c(3,2), echo=FALSE}
ggplot(data = useful_data %>% filter(win == 1), aes(x = STL)) +   geom_histogram(binwidth = 1) + 
  labs(x = "STL", 
       y = "Count", 
       title = "Steals Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = STL)) +   geom_histogram(binwidth = 1) + 
  labs(x = "STL", 
       y = "Count", 
       title = "Steals Distribution")
```

Summary of Steals:

```{r, echo=FALSE}
summary(useful_data$STL)
```

Distribution of Blocks:

```{r, fig.dim=c(3,2), echo=FALSE}
ggplot(data = useful_data %>% filter(win == 1), aes(x = BLK)) +   geom_histogram(binwidth = 1) + 
  labs(x = "BLK", 
       y = "Count", 
       title = "Block Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = BLK)) +   geom_histogram(binwidth = 1) + 
  labs(x = "BLK", 
       y = "Count", 
       title = "Block Distribution")
```

Summary of Blocks:

```{r, echo=FALSE}
summary(useful_data$BLK)
```

Distribution of Turnovers:

```{r, fig.dim=c(3,2), echo=FALSE}
ggplot(data = useful_data %>% filter(win == 1), aes(x = TOV)) +   geom_histogram(binwidth = 2) + 
  labs(x = "TOV", 
       y = "Count", 
       title = "Turnovers Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = TOV)) +   geom_histogram(binwidth = 2) + 
  labs(x = "TOV", 
       y = "Count", 
       title = "Turnovers Distribution")
```

Summary of Turnovers:

```{r, echo=FALSE}
summary(useful_data$TOV)
```

Distribution of Personal Fouls:

```{r, fig.dim=c(3,2), echo=FALSE}
ggplot(data = useful_data %>% filter(win == 1), aes(x = PF)) +   geom_histogram(binwidth = 2) + 
  labs(x = "PF", 
       y = "Count", 
       title = "Personal Foul Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = PF)) +   geom_histogram(binwidth = 2) + 
  labs(x = "PF", 
       y = "Count", 
       title = "Personal Foul Distribution")
```

Summary of Personal Fouls:

```{r, echo=FALSE}
summary(useful_data$PF)
```

## Residuals of Each Variable
Residual Plot of Field Goal Attempts:

```{r, fig.dim=c(3,2), echo=FALSE}
ggplot(data = model_data, aes(x=.resid, y=FGA)) + geom_point() +
  labs(x="Residuals",
       y="Field Goal Attemps",
       title="Residual Plot of Field Goal Attempts")
```

Residual Plot of Points:

```{r, fig.dim=c(3,2), echo=FALSE}
ggplot(data = model_data, aes(x=.resid, y=FGP)) + geom_point() +
  labs(x="Residuals",
       y="Field Goal Percentage",
       title="Residual Plot of Points")
```

Residual Plot of Three Pointers:

```{r, fig.dim=c(3,2), echo=FALSE}
ggplot(data = model_data, aes(x=.resid, y=TP)) + geom_point() +
  labs(x="Residuals",
       y="Three Pointers",
       title="Residual Plot of Three Pointers")
```

Residual Plot of Three Point Attempts:

```{r, fig.dim=c(3,2), echo=FALSE}
ggplot(data = model_data, aes(x=.resid, y=TPA)) + geom_point() +
  labs(x="Residuals",
       y="Three Point Attempts",
       title="Residual Plot of Three Point Attemps")
```

Residual Plot of Three Point Percentage:

```{r, fig.dim=c(3,2), echo=FALSE}
ggplot(data = model_data, aes(x=.resid, y=TPP)) + geom_point() +
  labs(x="Residuals",
       y="Three Point Percentage",
       title="Residual Plot of Three Point Percentage")
```

Residual Plot of Offensive Rebounds:

```{r, fig.dim=c(3,2), echo=FALSE}
ggplot(data = model_data, aes(x=.resid, y=ORB)) + geom_point() +
  labs(x="Residuals",
       y="Offensive Rebounds",
       title="Residual Plot of Offensive Rebounds")
```

Residual Plot of Defensive Rebounds:

```{r, fig.dim=c(3,2), echo=FALSE}
ggplot(data = model_data, aes(x=.resid, y=DRB)) + geom_point() +
  labs(x="Residuals",
       y="Defensive Rebounds",
       title="Residual Plot of Defensive Rebounds")
```

Residual Plot of Assists:

```{r, fig.dim=c(3,2), echo=FALSE}
ggplot(data = model_data, aes(x=.resid, y=AST)) + geom_point() +
  labs(x="Residuals",
       y="Assists",
       title="Residual Plot of Assists")
```

Residual Plot of Steals:

```{r, fig.dim=c(3,2), echo=FALSE}
ggplot(data = model_data, aes(x=.resid, y=STL)) + geom_point() +
  labs(x="Residuals",
       y="Steals",
       title="Residual Plot of Steals")
```

Residual Plot of Blocks:

```{r, fig.dim=c(3,2), echo=FALSE}
ggplot(data = model_data, aes(x=.resid, y=BLK)) + geom_point() +
  labs(x="Residuals",
       y="Blocks",
       title="Residual Plot of Blocks")
```

Residual Plot of Turnovers:

```{r, fig.dim=c(3,2), echo=FALSE}
ggplot(data = model_data, aes(x=.resid, y=TOV)) + geom_point() +
  labs(x="Residuals",
       y="Turnovers",
       title="Residual Plot of Turnovers")
```

Residual Plot of PF:

```{r, fig.dim=c(3,2), echo=FALSE}
ggplot(data = model_data, aes(x=.resid, y=PF)) + geom_point() +
  labs(x="Residuals",
       y="PF",
       title="Residual Plot of PF")
```
