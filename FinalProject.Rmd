---
title: "Finalproject"
author: "Ravi G and Rohit K"
date: "3/8/2022"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(broom)
library(pROC)
library(plotROC)
library(rms)
```
# Background:
Every year, the National Basketball Association (NBA) ends the season with a series of championship games between the two best teams in the league. The series is a best-of-7, where the first team to win 4 games is the winner of the series. The championship is a very important accolade, both teams and players are compared by the number of championships they have won. In these comparisons, statistics like field goal percentage, offensive rebounds, steals, and blocks can be used to determine a team’s performance and be an indicator of how
much better one team is than another. 

Statistics in the NBA are highly analyzed, very often before a championship game to try to predict the outcome of the game. Our group wants to create a model that will be able to predict whether or not a team wins the NBA championship given the team’s statistics during the NBA finals game. We want to find weights for each statistic that can show us how important each statistic is to predicting the overall winner, which helps our group conduct a greater analysis of how different focuses and strategies can affect the outcome of NBA games. Our group believes that certain statistics such as field goal percentage, turnovers, and offensive rebounds will be prevalent in winning NBA teams.

# Data:
The dataset with which we want to create our model is the NBA Finals Team Stats dataset on Kaggle uploaded by Dave Rosenman. The dataset contains final data from 1980 to 2018, and is divided into two tables. The first table contains the data of each winning team and
the second contains the losing team. Each observation includes data points like field goals made, field goals attempted, three point shots made, free throws made, total rebounds, assists,
steals, turnovers, blocks, and many other statistics that will be covered in the data summary. The data takes averages from each game in the series, and its an average of the performance of the team in this category across all the games played in the series.

The NBA Finals Team Stats Dataset has been analyzed and used to create models by several Kaggle Users. One project to note is a report written by Ziyu Liu (insert citation here) called "Three pointers win championships", in which the author creates a model to see if the number of three point shots made by a team can predict whether or not the team wins the championship. In the study, the model achieves an accuracy of 59%. This tells us that, while three point shots are important, more statistics are required to be able to create a more accurate model.

In order to create the dataset we are using in this study, we started with two separate datasets, one for all of the series winners (NBA Champions) and one of the runner-ups. We created a new column ```win``` with a 1 if the team won the series and a 0 if the team lost. This will be our predictor variable for the model. Next, we combined the two datsets and randomized the order of the entries. Our goal is to first analyze each of the variables to determine which will be the most useful in creating our model, then going through several iterations of models before choosing the most accurate one. 


These are the libraries that will be used to create this model:
```
library(tidyverse)
library(broom)
library(pROC)
library(plotROC)
library(rms)
```



## Exploratory Data Analysis

We started by creating the dataset we wish to use for this study (using the process mentioned in the Data section). 

```{r}
champs_data <- read_csv("data/champs_series_averages.csv")
runnerups_data <- read_csv("data/runner_ups_series_averages.csv")

```
```{r}
champs_data <- champs_data %>%
  mutate(win = "1")
runnerups_data <- runnerups_data %>%
  mutate(win = "0")
```
```{r}
all_data <- rbind(champs_data, runnerups_data)
```


Next, we eliminated some variables that we did not wish to explore or view the effect they would have on the model. This includes statistics like ```FTA``` (Free Throw Attempts), ```TPA``` (Three Point Attempts), ```BLK``` (Blocks). Some of these statistics describe the attempts to make a point, however the statistics describing how many points were made in that fashion would be a much more accurate tool in the model. Others simply do not happen often enough to quantifiable change the course of a championship series. 

```{r}
useful_data = subset(all_data, select = -c(...1,Year, Status, Team, FT, FTA, FTP, TRB) )
```


Now, we can start our EDA. First, we check the plots of each variable for both the losers and the winners to make sure each distribution is roughly normal. 

```{r, fig.dim=c(3,2)}
ggplot(data = useful_data %>% filter(win == 1), aes(x = PTS)) +   geom_histogram(binwidth = 4) + 
  labs(x = "Points", 
       y = "Count", 
       title = "Point Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = PTS)) +   geom_histogram(binwidth = 4) + 
  labs(x = "Points", 
       y = "Count", 
       title = "Point Distribution")
```
```{r, fig.dim=c(3,2)}
ggplot(data = useful_data %>% filter(win == 1), aes(x = FG)) +   geom_histogram(binwidth = 2) + 
  labs(x = "FG", 
       y = "Count", 
       title = "FG Made Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = FG)) +   geom_histogram(binwidth = 2) + 
  labs(x = "FG", 
       y = "Count", 
       title = "FG Made Distribution")
```
```{r, fig.dim=c(3,2)}
ggplot(data = useful_data %>% filter(win == 1), aes(x = FGA)) +   geom_histogram(binwidth = 3) + 
  labs(x = "FGA", 
       y = "Count", 
       title = "FG Attempted Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = FGA)) +   geom_histogram(binwidth = 3) + 
  labs(x = "FGA", 
       y = "Count", 
       title = "FG Attempted Distribution")
```
```{r, fig.dim=c(3,2)}
ggplot(data = useful_data %>% filter(win == 1), aes(x = FGP)) +   geom_histogram(binwidth = 2) + 
  labs(x = "FGP", 
       y = "Count", 
       title = "FG Percentage Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = FGP)) +   geom_histogram(binwidth = 2) + 
  labs(x = "FGP", 
       y = "Count", 
       title = "FG Percentage Distribution")
```
```{r, fig.dim=c(3,2)}
ggplot(data = useful_data %>% filter(win == 1), aes(x = TP)) +   geom_histogram(binwidth = 2) + 
  labs(x = "TP", 
       y = "Count", 
       title = "Three Point Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = TP)) +   geom_histogram(binwidth = 2) + 
  labs(x = "TP", 
       y = "Count", 
       title = "Three Point Distribution")
```
```{r, fig.dim=c(3,2)}
ggplot(data = useful_data %>% filter(win == 1), aes(x = TPA)) +   geom_histogram(binwidth = 3) + 
  labs(x = "TPA", 
       y = "Count", 
       title = "Point Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = TPA)) +   geom_histogram(binwidth = 3) + 
  labs(x = "TPA", 
       y = "Count", 
       title = "TP Attempted Distribution")
```
```{r, fig.dim=c(3,2)}
ggplot(data = useful_data %>% filter(win == 1), aes(x = TPP)) +   geom_histogram(binwidth = 4) + 
  labs(x = "TP Percentage", 
       y = "Count", 
       title = "TP Percentage Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = TPP)) +   geom_histogram(binwidth = 4) + 
  labs(x = "TP Percentage", 
       y = "Count", 
       title = "TP Percentage Distribution")
```
```{r, fig.dim=c(3,2)}
ggplot(data = useful_data %>% filter(win == 1), aes(x = ORB)) +   geom_histogram(binwidth = 2) + 
  labs(x = "ORB", 
       y = "Count", 
       title = "Offensive Rebounds Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = ORB)) +   geom_histogram(binwidth = 2) + 
  labs(x = "ORB", 
       y = "Count", 
       title = "Offensive Rebounds Distribution")
```
```{r, fig.dim=c(3,2)}
ggplot(data = useful_data %>% filter(win == 1), aes(x = DRB)) +   geom_histogram(binwidth = 2) + 
  labs(x = "DRB", 
       y = "Count", 
       title = "Defensive Rebound Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = DRB)) +   geom_histogram(binwidth = 2) + 
  labs(x = "DRB", 
       y = "Count", 
       title = "Defensive Rebound Distribution")
```
```{r, fig.dim=c(3,2)}
ggplot(data = useful_data %>% filter(win == 1), aes(x = AST)) +   geom_histogram(binwidth = 2) + 
  labs(x = "AST", 
       y = "Count", 
       title = "Assists Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = AST)) +   geom_histogram(binwidth = 2) + 
  labs(x = "AST", 
       y = "Count", 
       title = "Assists Distribution")
```
```{r, fig.dim=c(3,2)}
ggplot(data = useful_data %>% filter(win == 1), aes(x = STL)) +   geom_histogram(binwidth = 1) + 
  labs(x = "STL", 
       y = "Count", 
       title = "Steals Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = STL)) +   geom_histogram(binwidth = 1) + 
  labs(x = "STL", 
       y = "Count", 
       title = "Steals Distribution")
```
```{r, fig.dim=c(3,2)}
ggplot(data = useful_data %>% filter(win == 1), aes(x = BLK)) +   geom_histogram(binwidth = 1) + 
  labs(x = "BLK", 
       y = "Count", 
       title = "Block Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = BLK)) +   geom_histogram(binwidth = 1) + 
  labs(x = "BLK", 
       y = "Count", 
       title = "Block Distribution")
```
```{r, fig.dim=c(3,2)}
ggplot(data = useful_data %>% filter(win == 1), aes(x = TOV)) +   geom_histogram(binwidth = 2) + 
  labs(x = "TOV", 
       y = "Count", 
       title = "Turnovers Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = TOV)) +   geom_histogram(binwidth = 2) + 
  labs(x = "TOV", 
       y = "Count", 
       title = "Turnovers Distribution")
```
```{r, fig.dim=c(3,2)}
ggplot(data = useful_data %>% filter(win == 1), aes(x = PF)) +   geom_histogram(binwidth = 2) + 
  labs(x = "PF", 
       y = "Count", 
       title = "Personal Foul Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = PF)) +   geom_histogram(binwidth = 2) + 
  labs(x = "PF", 
       y = "Count", 
       title = "Personal Foul Distribution")
```


# Creating the Model

## Model Refinement

Now that we have checked that the distributions of the variables are somewhat normal, we can create a model and check the residuals. Our model will be a binomial model (only options are 0 ad 1). The first step will be to plot the residuals of each variable in the model to check the linearity assumption. Then, we will plot the Cook's distance and remove any high-leverage points. Finally, the VIF will be checked and any variables with a high VIF will be removed from the model.

```{r}

useful_data$win <- as.factor(useful_data$win)

model <- glm(win ~ PTS + FG + FGA + FGP + TP + TPA + TPP + ORB + DRB + AST + STL + BLK + TOV + PF, useful_data, family=binomial)

summary(model)
```

```{r}

model_data <- augment(model, useful_data)
head(model_data)
```

```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=.resid, y=PTS)) + geom_point() +
  labs(x="Residuals",
       y="Points",
       title="Residual Plot of Points")
```
```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=.resid, y=FG)) + geom_point() +
  labs(x="Residuals",
       y="Field Goals",
       title="Residual Plot of Field Goals")
```
```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=.resid, y=FGA)) + geom_point() +
  labs(x="Residuals",
       y="Field Goal Attemps",
       title="Residual Plot of Field Goal Attempts")
```
```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=.resid, y=FGP)) + geom_point() +
  labs(x="Residuals",
       y="Field Goal Percentage",
       title="Residual Plot of Points")
```
```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=.resid, y=TP)) + geom_point() +
  labs(x="Residuals",
       y="Three Pointers",
       title="Residual Plot of Three Pointers")
```
```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=.resid, y=TPA)) + geom_point() +
  labs(x="Residuals",
       y="Three Point Attempts",
       title="Residual Plot of Three Point Attemps")
```
```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=.resid, y=TPP)) + geom_point() +
  labs(x="Residuals",
       y="Three Point Percentage",
       title="Residual Plot of Three Point %")
```
```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=.resid, y=ORB)) + geom_point() +
  labs(x="Residuals",
       y="Offensive Rebounds",
       title="Residual Plot of Offensive Rebounds")
```
```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=.resid, y=DRB)) + geom_point() +
  labs(x="Residuals",
       y="Defensive Rebounds",
       title="Residual Plot of Defensive Rebounds")
```
```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=.resid, y=AST)) + geom_point() +
  labs(x="Residuals",
       y="Assists",
       title="Residual Plot of Assists")
```
```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=.resid, y=STL)) + geom_point() +
  labs(x="Residuals",
       y="Steals",
       title="Residual Plot of Steals")
```
```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=.resid, y=BLK)) + geom_point() +
  labs(x="Residuals",
       y="Blocks",
       title="Residual Plot of Blocks")
```
```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=.resid, y=TOV)) + geom_point() +
  labs(x="Residuals",
       y="Turnovers",
       title="Residual Plot of Turnovers")
```
```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=.resid, y=PF)) + geom_point() +
  labs(x="Residuals",
       y="PF",
       title="Residual Plot of PF")
```

The residual plots for each variable appear to be random and evenly dispersed, which means that the linearity assumption is satisfied. Before we can test the accuracy of the model, we must also explore how these observations affect the model, and how the variables used in the model affect each other. First, we can plot the leverage (.cooksd) of each observation to see if there are any high-leverage data points. 

```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=seq.int(nrow(model_data)), y=.cooksd)) + geom_point() + 
  labs(x="Observation",
       y="Cook's Distance",
       title="Cook's Distance of each Observation") + 
  geom_hline(yintercept=0.125)
```

It is obvious that there are two high-leverage data points. If we use a threshold of 0.125, we can eliminate these two high-leverage points to make the model better at prediction. After this, the model must be trained on the newly-filtered data.

```{r}

filter_data <- filter(model_data, .cooksd < 0.125)

filter_data <- select(filter_data, 1:15)

filter_model <- glm(win ~ PTS + FG + FGA + FGP + TP + TPA + TPP + ORB + DRB + AST + STL + BLK + TOV + PF, filter_data, family=binomial)

summary(filter_model)

filter_data <- augment(filter_model, filter_data)

```

We must first re_train the model on the filter data The next step is to check how the variables interact with each other. To measure this, we want to calculate the Variable Inflation Factor, or $VIF$. 

```{r}

vif(filter_model)

```

These values for VIF are extremely high because there are variables that measure similar statistics. However, because there are columns like ```FG```, ```FGA```, and ```FGP``` which represent field goals made, field goal attempts, and field goal percentage; a high VIF is not a concern for these variables. The same logic can be applied to ```TP```, ```TPA``` and ```TPP```, which measure the three point attempts and the three point percentage. Therefore, we will proceed with measuring the success of the model. 

## Model Assessment and Prediction

