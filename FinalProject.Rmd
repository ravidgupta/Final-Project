---
title: "Predicting NBA Championship Winners Using Data from Past Championship Series"
author: "Ravi G and Rohit K"
date: "3/8/2022"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(broom)
library(pROC)
library(plotROC)
library(rms)
library(caret)
```
# Background:
Every year, the National Basketball Association (NBA) ends the season with a series of championship games between the two best teams in the league. The series is a best-of-7, where the first team to win 4 games is the winner of the series. The championship is a very important accolade, both teams and players are compared by the number of championships they have won. In these comparisons, statistics like field goal percentage, offensive rebounds, steals, and blocks can be used to determine a team’s performance and be an indicator of how
much better one team is than another. 

Statistics in the NBA are highly analyzed, very often before a championship game to try to predict the outcome of the game. Our group wants to create a model that will be able to predict whether or not a team wins the NBA championship given the team’s statistics during the NBA finals game. We want to find weights for each statistic that can show us how important each statistic is to predicting the overall winner, which helps our group conduct a greater analysis of how different focuses and strategies can affect the outcome of NBA games. Our group believes that certain statistics such as field goal percentage, turnovers, and offensive rebounds will be prevalent in winning NBA teams.

# Data:
The dataset with which we want to create our model is the NBA Finals Team Stats dataset on Kaggle uploaded by Dave Rosenman. The dataset contains final data from 1980 to 2018, and is divided into two tables. The first table contains the data of each winning team and
the second contains the losing team. Each observation includes data points like field goals made, field goals attempted, three point shots made, free throws made, total rebounds, assists,
steals, turnovers, blocks, and many other statistics that will be covered in the data summary. The data takes averages from each game in the series, and its an average of the performance of the team in this category across all the games played in the series.

The NBA Finals Team Stats Dataset has been analyzed and used to create models by several Kaggle Users. One project to note is a report written by Ziyu Liu (insert citation here) called "Three pointers win championships", in which the author creates a model to see if the number of three point shots made by a team can predict whether or not the team wins the championship. In the study, the model achieves an accuracy of 59%. This tells us that, while three point shots are important, more statistics are required to be able to create a more accurate model.

In order to create the dataset we are using in this study, we started with two separate datasets, one for all of the series winners (NBA Champions) and one of the runner-ups. We created a new column ```win``` with a 1 if the team won the series and a 0 if the team lost. This will be our predictor variable for the model. Next, we combined the two datsets and randomized the order of the entries. Our goal is to first analyze each of the variables to determine which will be the most useful in creating our model, then going through several iterations of models before choosing the most accurate one. 


These are the libraries that will be used to create this model:
```
library(tidyverse)
library(broom)
library(pROC)
library(plotROC)
library(rms)
library(caret)
```



## Exploratory Data Analysis

We started by creating the dataset we wish to use for this study (using the process mentioned in the Data section). 

```{r}
champs_data <- read_csv("data/champs_series_averages.csv")
runnerups_data <- read_csv("data/runner_ups_series_averages.csv")

```
```{r}
champs_data <- champs_data %>%
  mutate(win = "1")
runnerups_data <- runnerups_data %>%
  mutate(win = "0")
```
```{r}
all_data <- rbind(champs_data, runnerups_data)
```


Next, we eliminated some variables that we did not wish to explore or view the effect they would have on the model. This includes statistics like ```FTA``` (Free Throw Attempts), ```TPA``` (Three Point Attempts), ```BLK``` (Blocks). Some of these statistics describe the attempts to make a point, however the statistics describing how many points were made in that fashion would be a much more accurate tool in the model. Others simply do not happen often enough to quantifiable change the course of a championship series. 

```{r}
useful_data = subset(all_data, select = -c(...1,Year, Status, Team, FT, FTA, FTP, TRB) )
summary(useful_data$FGA)
```


Now, we can start our EDA. First, we check the plots of each variable for both the losers and the winners to make sure each distribution is roughly normal. 

```{r, fig.dim=c(3,2)}
ggplot(data = useful_data %>% filter(win == 1), aes(x = PTS)) +   geom_histogram(binwidth = 4) + 
  labs(x = "Points", 
       y = "Count", 
       title = "Point Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = PTS)) +   geom_histogram(binwidth = 4) + 
  labs(x = "Points", 
       y = "Count", 
       title = "Point Distribution")
```

Summary of Points:
```{r}
summary(useful_data$PTS)
```

```{r, fig.dim=c(3,2)}
ggplot(data = useful_data %>% filter(win == 1), aes(x = FG)) +   geom_histogram(binwidth = 2) + 
  labs(x = "FG", 
       y = "Count", 
       title = "FG Made Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = FG)) +   geom_histogram(binwidth = 2) + 
  labs(x = "FG", 
       y = "Count", 
       title = "FG Made Distribution")
```
Summary of Field Goals:
```{r}
summary(useful_data$FG)
```
```{r, fig.dim=c(3,2)}
ggplot(data = useful_data %>% filter(win == 1), aes(x = FGA)) +   geom_histogram(binwidth = 3) + 
  labs(x = "FGA", 
       y = "Count", 
       title = "FG Attempted Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = FGA)) +   geom_histogram(binwidth = 3) + 
  labs(x = "FGA", 
       y = "Count", 
       title = "FG Attempted Distribution")
```

The distributions and summaries of the other variables can be seen in the appendix. All of the distributions appear to be somewhat normal, so we can continue to make our model and check the residuals.


# Creating the Model

## Model Refinement

Our model will be a binomial model (only options are 0 ad 1). The first step will be to plot the residuals of each variable in the model to check the linearity assumption. Then, we will plot the Cook's distance and remove any high-leverage points. Finally, the VIF will be checked and any variables with a high VIF will be removed from the model.

```{r}

useful_data$win <- as.factor(useful_data$win)

model <- glm(win ~ PTS + FG + FGA + FGP + TP + TPA + TPP + ORB + DRB + AST + STL + BLK + TOV + PF, useful_data, family=binomial)

summary(model)
```

```{r}

model_data <- augment(model, useful_data)
head(model_data)
```

```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=.resid, y=PTS)) + geom_point() +
  labs(x="Residuals",
       y="Points",
       title="Residual Plot of Points")
```
```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=.resid, y=FG)) + geom_point() +
  labs(x="Residuals",
       y="Field Goals",
       title="Residual Plot of Field Goals")
```

The residual plots for each variable appear to be random and evenly dispersed (the rest can be seen in the appendix), which means that the linearity assumption is satisfied. Before we can test the accuracy of the model, we must also explore how these observations affect the model, and how the variables used in the model affect each other. First, we can plot the leverage (.cooksd) of each observation to see if there are any high-leverage data points. 

```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=seq.int(nrow(model_data)), y=.cooksd)) + geom_point() + 
  labs(x="Observation",
       y="Cook's Distance",
       title="Cook's Distance of each Observation") + 
  geom_hline(yintercept=0.125)
```

It is obvious that there are two high-leverage data points. If we use a threshold of 0.125, we can eliminate these two high-leverage points to make the model better at prediction. After this, the model must be trained on the newly-filtered data.

```{r}

filter_data <- filter(model_data, .cooksd < 0.125)

filter_data <- select(filter_data, 1:15)

filter_model <- glm(win ~ PTS + FG + FGA + FGP + TP + 
                      TPA + TPP + ORB + DRB + AST + STL + BLK + TOV + PF, filter_data, family=binomial)

summary(filter_model)

filter_data <- augment(filter_model, filter_data)

```

We must first re_train the model on the filter data The next step is to check how the variables interact with each other. To measure this, we want to calculate the Variable Inflation Factor, or $VIF$. 

```{r}

vif(filter_model)

```

Some of the variables have a very high $VIF$, so we can only include certain variables to keep the score lower. To see which variables are better included and not included, we must create multiple models and assess which one has the best accuracy. We can leave all of the variables whos $VIF$ is lower than 10, but the others must be removed for higher accuracy. We will create three models, one that will include only the Field Goals made and the Three Point shots made; one that will only include the percentage of Field Goals and Three Point shots made; and finally an attemps model that will include the number of Field Goal attemps and Three Point attempts. 

```{r}

points_model <- glm(win ~ PTS + FG + TP + ORB + DRB + AST + STL + BLK + TOV + PF, 
                    data = filter_data, na.action = na.omit, family = binomial)

percentage_model <- glm(win ~ PTS + FGP + TPP + ORB + DRB + AST + STL + BLK + TOV + PF, 
                    data = filter_data, na.action = na.omit, family = binomial)

attempts_model <- glm(win ~ PTS + FGA + TPA + ORB + DRB + AST + STL + BLK + TOV + PF, 
                    data = filter_data, na.action = na.omit, family = binomial)
```

To evaluate the accuracy of each model, we will train each model to our dataset to make predictions and measure the accuracy of these predictions. To do this, for each model we will plot the ROC curve, find the ideal threshold of each model, create predictions, then measure the accuracy. 

```{r}

filter_data <- select(filter_data, 1:15)
points_data <- filter_data
percentage_data <- filter_data
attempts_data <- filter_data
```

Points Model:
```{r, fig.dim=c(3,2)}

points_data <- augment(points_model, points_data)
points_roc <- roc(points_data, win, .fitted, plot=TRUE)

```
```{r}

threshold <- coords(points_roc, "best", ret = "threshold")
print(threshold)
```

The ideal threshold for the points model has been shown as ```-0.1734443```. Using this threshold, we can create a confusion matrix and draw conclusions about the accuracy of the model. 

```{r}
points_data <- mutate(points_data, pred = ifelse(.fitted > -0.1734443, 1, 0))

points_data$pred <- as.factor(points_data$pred)

confusionMatrix(points_data$pred, points_data$win)

```

The accuracy of the points model was found to be 74.32%. Using this same method for the other models, we found that the accuracy of the percentage model was 81.08% and the accuracy of the attempts model was 82.43%. In this case, solely using the attempts of field goals and three pointers made the model more accurate. We will use the attempts model as our current model for the next step of tests.

```{r}
current_model <- attempts_model
```

To continue refining the model, we will test how including the ```PTS``` variable affects the accuracy of our model.








# Conclusion

```{r}
summary(final_model)
```

By using this model, we can predict whether or not an NBA team won a finals series given their statistics from the series with an accuracy of 87.64%, making the model useful (better than guessing win or lose for each year). To refine this model more, we can perform the same analysis on all 22 variables given from the original data set, and have a solution for the high $VIF$ found due to the similarity of some measurements during games. 

## Observations

We found it interesting that the attempts of field goals and three pointers provided the most accurate model for predicting who won the series. It intuitively makes sense that the field goals scored matter more than attempts because basketball games are decided by the score, not the attempts; however, attempts might signal how effective a team's offense really can be. When a team can get in scoring position enough times to have a high number of attempts, that will help the team's chances of winning whether or not those shots are actually made. 

It is worth noting that, while the coefficients for most variables make sense (ie. turnovers having a negative coefficient and field goals having a positive coefficient), others have a surprising effect on the model. 

For example, the coefficients for points (```PTS```) and assists (```AST```) are both negative, implying that more points and more assists indicate a lesser likelihood that a team won the game. This can be attributed to the fact that basketball is a game that cannot be won by statistics: even if a team scores a high number of points, a bad defense can still lose them the series. 

Some variables with high coefficients include: Offensive Rebounds (```ORB```), Steals (```STL```), and Field Goals Made(```FG```). These all seem to be good at indicating whether or not teams won the series, which is fascinating because they cover different aspects of the game. A team cannot solely rely on shooting, defense, or size (in the case of offensive rebounds) to win a championship; they must have all aspects of the game.




# Appendix

## Distributions of variables for Winners and Losers
Summary of Field Goal Attempts:

```{r, fig.dim=c(3,2)}
ggplot(data = useful_data %>% filter(win == 1), aes(x = FGP)) +   geom_histogram(binwidth = 2) + 
  labs(x = "FGP", 
       y = "Count", 
       title = "FG Percentage Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = FGP)) +   geom_histogram(binwidth = 2) + 
  labs(x = "FGP", 
       y = "Count", 
       title = "FG Percentage Distribution")
```
Summary of Field Goal Percentage:
```{r}
summary(useful_data$FGP)
```
```{r, fig.dim=c(3,2)}
ggplot(data = useful_data %>% filter(win == 1), aes(x = TP)) +   geom_histogram(binwidth = 2) + 
  labs(x = "TP", 
       y = "Count", 
       title = "Three Point Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = TP)) +   geom_histogram(binwidth = 2) + 
  labs(x = "TP", 
       y = "Count", 
       title = "Three Point Distribution")
```
Summary of Three Pointers:
```{r}
summary(useful_data$TP)
```
```{r, fig.dim=c(3,2)}
ggplot(data = useful_data %>% filter(win == 1), aes(x = TPA)) +   geom_histogram(binwidth = 3) + 
  labs(x = "TPA", 
       y = "Count", 
       title = "Point Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = TPA)) +   geom_histogram(binwidth = 3) + 
  labs(x = "TPA", 
       y = "Count", 
       title = "TP Attempted Distribution")
```
Summary of Three Point Attempts:
```{r}
summary(useful_data$TPA)
```
```{r, fig.dim=c(3,2)}
ggplot(data = useful_data %>% filter(win == 1), aes(x = TPP)) +   geom_histogram(binwidth = 4) + 
  labs(x = "TP Percentage", 
       y = "Count", 
       title = "TP Percentage Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = TPP)) +   geom_histogram(binwidth = 4) + 
  labs(x = "TP Percentage", 
       y = "Count", 
       title = "TP Percentage Distribution")
```
Summary of Three Point Percentage:
```{r}
summary(useful_data$TPP)
```
```{r, fig.dim=c(3,2)}
ggplot(data = useful_data %>% filter(win == 1), aes(x = ORB)) +   geom_histogram(binwidth = 2) + 
  labs(x = "ORB", 
       y = "Count", 
       title = "Offensive Rebounds Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = ORB)) +   geom_histogram(binwidth = 2) + 
  labs(x = "ORB", 
       y = "Count", 
       title = "Offensive Rebounds Distribution")
```
Summary of Offensive Rebounds:
```{r}
summary(useful_data$ORB)
```
```{r, fig.dim=c(3,2)}
ggplot(data = useful_data %>% filter(win == 1), aes(x = DRB)) +   geom_histogram(binwidth = 2) + 
  labs(x = "DRB", 
       y = "Count", 
       title = "Defensive Rebound Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = DRB)) +   geom_histogram(binwidth = 2) + 
  labs(x = "DRB", 
       y = "Count", 
       title = "Defensive Rebound Distribution")
```
Summary of Defensive Rebounds:
```{r}
summary(useful_data$DRB)
```
```{r, fig.dim=c(3,2)}
ggplot(data = useful_data %>% filter(win == 1), aes(x = AST)) +   geom_histogram(binwidth = 2) + 
  labs(x = "AST", 
       y = "Count", 
       title = "Assists Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = AST)) +   geom_histogram(binwidth = 2) + 
  labs(x = "AST", 
       y = "Count", 
       title = "Assists Distribution")
```
Summary of Assists:
```{r}
summary(useful_data$AST)
```
```{r, fig.dim=c(3,2)}
ggplot(data = useful_data %>% filter(win == 1), aes(x = STL)) +   geom_histogram(binwidth = 1) + 
  labs(x = "STL", 
       y = "Count", 
       title = "Steals Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = STL)) +   geom_histogram(binwidth = 1) + 
  labs(x = "STL", 
       y = "Count", 
       title = "Steals Distribution")
```
Summary of Steals:
```{r}
summary(useful_data$STL)
```
```{r, fig.dim=c(3,2)}
ggplot(data = useful_data %>% filter(win == 1), aes(x = BLK)) +   geom_histogram(binwidth = 1) + 
  labs(x = "BLK", 
       y = "Count", 
       title = "Block Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = BLK)) +   geom_histogram(binwidth = 1) + 
  labs(x = "BLK", 
       y = "Count", 
       title = "Block Distribution")
```
Summary of Blocks:
```{r}
summary(useful_data$BLK)
```
```{r, fig.dim=c(3,2)}
ggplot(data = useful_data %>% filter(win == 1), aes(x = TOV)) +   geom_histogram(binwidth = 2) + 
  labs(x = "TOV", 
       y = "Count", 
       title = "Turnovers Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = TOV)) +   geom_histogram(binwidth = 2) + 
  labs(x = "TOV", 
       y = "Count", 
       title = "Turnovers Distribution")
```
Summary of Turnovers:
```{r}
summary(useful_data$TOV)
```
```{r, fig.dim=c(3,2)}
ggplot(data = useful_data %>% filter(win == 1), aes(x = PF)) +   geom_histogram(binwidth = 2) + 
  labs(x = "PF", 
       y = "Count", 
       title = "Personal Foul Distribution")
ggplot(data = useful_data %>% filter(win == 0), aes(x = PF)) +   geom_histogram(binwidth = 2) + 
  labs(x = "PF", 
       y = "Count", 
       title = "Personal Foul Distribution")
```
Summary of Personal Fouls:
```{r}
summary(useful_data$PF)
```

## Residuals of Each Variable

```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=.resid, y=FGA)) + geom_point() +
  labs(x="Residuals",
       y="Field Goal Attemps",
       title="Residual Plot of Field Goal Attempts")
```
```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=.resid, y=FGP)) + geom_point() +
  labs(x="Residuals",
       y="Field Goal Percentage",
       title="Residual Plot of Points")
```
```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=.resid, y=TP)) + geom_point() +
  labs(x="Residuals",
       y="Three Pointers",
       title="Residual Plot of Three Pointers")
```
```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=.resid, y=TPA)) + geom_point() +
  labs(x="Residuals",
       y="Three Point Attempts",
       title="Residual Plot of Three Point Attemps")
```
```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=.resid, y=TPP)) + geom_point() +
  labs(x="Residuals",
       y="Three Point Percentage",
       title="Residual Plot of Three Point %")
```
```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=.resid, y=ORB)) + geom_point() +
  labs(x="Residuals",
       y="Offensive Rebounds",
       title="Residual Plot of Offensive Rebounds")
```
```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=.resid, y=DRB)) + geom_point() +
  labs(x="Residuals",
       y="Defensive Rebounds",
       title="Residual Plot of Defensive Rebounds")
```
```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=.resid, y=AST)) + geom_point() +
  labs(x="Residuals",
       y="Assists",
       title="Residual Plot of Assists")
```
```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=.resid, y=STL)) + geom_point() +
  labs(x="Residuals",
       y="Steals",
       title="Residual Plot of Steals")
```
```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=.resid, y=BLK)) + geom_point() +
  labs(x="Residuals",
       y="Blocks",
       title="Residual Plot of Blocks")
```
```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=.resid, y=TOV)) + geom_point() +
  labs(x="Residuals",
       y="Turnovers",
       title="Residual Plot of Turnovers")
```
```{r, fig.dim=c(3,2)}
ggplot(data = model_data, aes(x=.resid, y=PF)) + geom_point() +
  labs(x="Residuals",
       y="PF",
       title="Residual Plot of PF")
```


## Additional Models

Percentage Model:
```{r, fig.dim=c(3,2)}

percentage_data <- augment(percentage_model, percentage_data)
percentage_roc <- roc(percentage_data, win, .fitted, plot=TRUE)

```
```{r}

threshold <- coords(percentage_roc, "best", ret = "threshold")
print(threshold)
```

The ideal threshold for the percentage model has been shown as ```-0.6158464```. Using this threshold, we can create a confusion matrix and draw conclusions about the accuracy of the model. 

```{r}
percentage_data <- mutate(percentage_data, pred = ifelse(.fitted > -0.6158464, 1, 0))

percentage_data$pred <- as.factor(percentage_data$pred)

confusionMatrix(percentage_data$pred, percentage_data$win)

```



Attempts Model:
```{r, fig.dim=c(3,2)}

attempts_data <- augment(attempts_model, attempts_data)
attempts_roc <- roc(attempts_data, win, .fitted, plot=TRUE)

```
```{r}

threshold <- coords(attempts_roc, "best", ret = "threshold")
print(threshold)
```

The ideal threshold for the percentage model has been shown as ```0.8960773```. Using this threshold, we can create a confusion matrix and draw conclusions about the accuracy of the model. 

```{r}
attempts_data <- mutate(attempts_data, pred = ifelse(.fitted > 0.8960773, 1, 0))

attempts_data$pred <- as.factor(attempts_data$pred)

confusionMatrix(attempts_data$pred, attempts_data$win)

```
